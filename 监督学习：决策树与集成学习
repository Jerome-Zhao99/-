1 决策树
本章将介绍决策树，一种简单而灵活的算法。我们首先将给出决策树的非线性与基于区域的特征，然后对基于区域的损失函数进行定义与对比，最后给出这些方法的优缺点（进而引出集成方法）。
1.1 非线性
决策树是一种天生支持「非线性」的机器学习算法。正式来说，如果一个方法是线性的，则其输入 x \in \mathbb{R}^n 会输出下面形式的假设函数：
h(x)=\theta^{T}x \\
其中 \theta \in \mathbb{R}^{n}，截距项内置（x_0 = 1）。
不能被化简为上述形式的假设函数即称为非线性。如果一个方法输出非线性的假设函数，则该方法也为非线性。之前我们介绍了对线性方法引入核函数，通过特征映射可以得到非线性的假设函数。
与核方法相比，决策树则可以直接输出非线性的假设函数。下图给出了基于时间与地点判断该地区能否滑雪的数据：
​

编辑

切换为居中
添加图片注释，不超过 140 字（可选）
可以看到该数据无法找出线性决策边界，但是我们可以将输入空间 \mathcal{X} 划分为不同的区域 R_i：
\begin{aligned} \mathcal{X} &=\bigcup_{i=0}^{n} R_{i} \\ \text { s.t. } & R_{i} \cap R_{j}=\emptyset \text { for } i \neq j \end{aligned} \\
1.2 选择区域
一般来说，选择最优的区域是较困难的。决策树通过「贪婪、自顶向下，递归的分割」来进行区域的选择。
具体来说，我们首先从原始的输入空间 \mathcal{X} 开始，基于单个特征的某个阈值将其划分为两个子区域，然后再选择其中一个子区域，基于新的阈值进行划分。我们持续以这种递归的方式训练模型：选择「叶子节点」（区域）、「特征」和「阈值」来形成一次新的分割。
形式上说，给定一个父区域 R_p，一个特征索引 j 和一个阈值 t \in \mathbb{R}，我们可以得到如下的两个子区域 R_1 和 R_2：
\begin{aligned}R_{1} &=\left\{X | X_{j} < t, X \in R_{p}\right\} \\ R_{2} &=\left\{X | X_{j} \geq t, X \in R_{p}\right\}\end{aligned} \\
对于之前的滑雪数据集，决策树的执行过程如下图所示：
​

编辑

切换为居中
添加图片注释，不超过 140 字（可选）
这一过程将持续至满足某个停止条件（之后细说），然后对每个叶子区域预测其所属类别。
1.3 定义一个损失函数
对于上面的过程，一个自然的问题是如何选择「分割」。我们可以基于区域集来定义损失函数 L。
给定一个父区域 R_p 和两个子区域 R_1 和 R_2，我们可以计算父区域的损失 L(R_p) 和子区域的基数加权损失：
\frac{\left|R_{1}\right| L\left(R_{1}\right)+\left|R_{2}\right| L\left(R_{2}\right)}{\left|R_{1}\right|+\left|R_{2}\right|} \\
在我们的「贪婪」分割框架中，我们希望选择区域、特征和阈值来使得损失减少最大化：
L\left(R_{p}\right)-\frac{\left|R_{1}\right| L\left(R_{1}\right)+\left|R_{2}\right| L\left(R_{2}\right)}{\left|R_{1}\right|+\left|R_{2}\right|} \\
1.3.1 错误分类损失
对于一个分类问题，我们感兴趣的是「错误分类的损失」 L_{misclass}。对于一个区域 R，令 \hat{p}_{c} 为区域中类别为 c 的样本比例，则 R 的错误分类损失可以写作：
L_{misclass}(R)=1-\max _{c}\left(\hat{p}_{c}\right) \\
该公式可以理解为我们将区域中样本数量最多的类作为希望该区域分割出的类别，则剩余的样本即为错误分类的样本。
然而，虽然错误分类损失是分类好坏的最终体现，但其对类别的概率并不敏感。下面的二分类例子体现了这一点：
​

编辑

切换为居中
添加图片注释，不超过 140 字（可选）
左侧的分割方式可以孤立出更多的正样本，看上去更好，但是：
L\left(R_{p}\right)=\frac{\left|R_{1}\right| L\left(R_{1}\right)+\left|R_{2}\right| L\left(R_{2}\right)}{\left|R_{1}\right|+\left|R_{2}\right|}=\frac{\left|R_{1}^{\prime}\right| L\left(R_{1}^{\prime}\right)+\left|R_{2}^{\prime}\right| L\left(R_{2}^{\prime}\right)}{\left|R_{1}^{\prime}\right|+\left|R_{2}^{\prime}\right|}= \frac 1 5 \\
可以看到，在错误分类损失函数下，两种分割方式的损失相同，而且也不能降低父区域的损失。
1.3.2 交叉熵损失
因此，我们需要定义一个更加敏感的损失函数，这里将使用「交叉熵损失」 L_{cross}：
L_{cross}(R)=-\sum_{c} \hat{p}_{c} \log _{2} \hat{p}_{c} \\
其中 \hat{p} \log _{2} \hat{p} \equiv 0 \text { if } \hat{p}=0。
从信息论的角度看，交叉熵测量了在分布已知的情况下，为了特定输出所需要的比特的数量。进一步来说，从父区域到子区域的损失的减少可以看做是信息的增加。
为了理解上述两种损失函数之间的敏感性的差别，我们将上面的二分类的例子的损失函数进行作图表述。对于该例，我们可以将损失函数简化为仅依赖于正样本 \hat{p}_i （在区域 R_i 中）
\begin{array}{l}{L_{misclass}(R)=L_{misclass}(\hat{p})=1-\max (\hat{p}, 1-\hat{p})} \\ {L_{cross}(R)=L_{cross}(\hat{p})=-\hat{p} \log \hat{p}-(1-\hat{p}) \log (1-\hat{p})}\end{array} \\
两个函数的图像如下：
​

编辑

切换为居中
添加图片注释，不超过 140 字（可选）

